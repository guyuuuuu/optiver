{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'C:/Users/gybj0/Desktop/data'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "\n",
    "train = pd.read_csv(os.path.join(DATA_DIR, 'optiver_volatility', 'train.csv'))\n",
    "stock_ids = set(train['stock_id'])\n",
    "\n",
    "def load_stock_data(stock_id: int, directory: str) -> pd.DataFrame:\n",
    "    return pd.read_parquet(os.path.join(DATA_DIR, 'optiver_volatility', directory, f'stock_id={stock_id}'))\n",
    "\n",
    "def load_book(stock_id: int, type:  str) -> pd.DataFrame:\n",
    "    return load_stock_data(stock_id, f'book_{type}.parquet')\n",
    "\n",
    "def load_trade(stock_id: int, type: str) -> pd.DataFrame:\n",
    "    return load_stock_data(stock_id, f'trade_{type}.parquet')\n",
    "\n",
    "def calc_wap1(df: pd.DataFrame) -> pd.Series:\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap2(df: pd.DataFrame) -> pd.Series:\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "def log_return(series: np.ndarray):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "def log_return_df2(series: np.ndarray):\n",
    "    return np.log(series).diff(2)\n",
    "\n",
    "def make_book_feature(stock_id: int, type: str) -> pd.DataFrame:\n",
    "    book = load_book(stock_id, type)\n",
    "\n",
    "    book['wap1'] = calc_wap1(book)\n",
    "    book['wap2'] = calc_wap2(book)\n",
    "    book['log_return1'] = book.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    book['log_return2'] = book.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    book['log_return_ask1'] = book.groupby(['time_id'])['ask_price1'].apply(log_return)\n",
    "    book['log_return_ask2'] = book.groupby(['time_id'])['ask_price2'].apply(log_return)\n",
    "    book['log_return_bid1'] = book.groupby(['time_id'])['bid_price1'].apply(log_return)\n",
    "    book['log_return_bid2'] = book.groupby(['time_id'])['bid_price2'].apply(log_return)\n",
    "\n",
    "    book['wap_balance'] = abs(book['wap1'] - book['wap2'])\n",
    "    book['price_spread'] = (book['ask_price1'] - book['bid_price1']) / ((book['ask_price1'] + book['bid_price1']) / 2)\n",
    "    book['bid_spread'] = book['bid_price1'] - book['bid_price2']\n",
    "    book['ask_spread'] = book['ask_price1'] - book['ask_price2']\n",
    "    book['total_volume'] = (book['ask_size1'] + book['ask_size2']) + (book['bid_size1'] + book['bid_size2'])\n",
    "    book['volume_imbalance'] = abs((book['ask_size1'] + book['ask_size2']) - (book['bid_size1'] + book['bid_size2']))\n",
    "\n",
    "    features = {\n",
    "        'seconds_in_bucket': ['count'],\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_ask1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_ask2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_bid1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_bid2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'wap_balance': [np.sum, np.mean, np.std],\n",
    "        'price_spread':[np.sum, np.mean, np.std],\n",
    "        'bid_spread':[np.sum, np.mean, np.std],\n",
    "        'ask_spread':[np.sum, np.mean, np.std],\n",
    "        'total_volume':[np.sum, np.mean, np.std],\n",
    "        'volume_imbalance':[np.sum, np.mean, np.std]\n",
    "    }\n",
    "    \n",
    "    book = book.fillna(method = 'ffill').dropna()\n",
    "    agg = book.groupby('time_id').agg(features).reset_index(drop=False)\n",
    "    agg.columns = ['_'.join(col) for col in agg.columns]\n",
    "    agg['stock_id'] = stock_id   \n",
    "    return agg\n",
    "\n",
    "def make_trade_feature(stock_id: int, type: str) -> pd.DataFrame:\n",
    "    trade = load_trade(stock_id, type)\n",
    "    trade['log_return'] = trade.groupby('time_id')['price'].apply(log_return)\n",
    "\n",
    "    features = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':['count'],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.mean],\n",
    "    }\n",
    "\n",
    "    agg = trade.groupby('time_id').agg(features).reset_index()\n",
    "    agg.columns = ['_'.join(col) for col in agg.columns]\n",
    "    agg['stock_id'] = stock_id\n",
    "    return agg\n",
    "\n",
    "def make_features(stock_ids, type):\n",
    "    from joblib import Parallel, delayed\n",
    "    book = Parallel(n_jobs = -1)(delayed(make_book_feature)(i, type) for i in stock_ids)\n",
    "    books = pd.concat(book)\n",
    "    trade = Parallel(n_jobs = -1)(delayed(make_trade_feature)(i, type) for i in stock_ids)\n",
    "    trades = pd.concat(trade)\n",
    "    features = pd.merge(books, trades, how = 'inner', on = ['stock_id','time_id_'])\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = make_features(stock_ids, 'train')\n",
    "\n",
    "test = pd.read_csv(os.path.join(DATA_DIR, 'optiver_volatility','test.csv'))\n",
    "test_ids = set(test.stock_id)\n",
    "\n",
    "test_features = make_features(test_ids, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.130823988098436\n",
      "0.0006416819842793221\n"
     ]
    }
   ],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "trains=pd.merge(train_features, train, left_on = ['time_id_','stock_id'], right_on = ['time_id','stock_id'], how = 'inner')\n",
    "\n",
    "drop_col = ['target','time_id','time_id_','stock_id']\n",
    "train_X = trains[trains.time_id_ < 32767 * 0.8].loc[:, ~trains.columns.isin(drop_col)]\n",
    "train_y = trains[trains.time_id_ < 32767 * 0.8].target\n",
    "\n",
    "test_X = trains[trains.time_id_ > 32767 * 0.8].loc[:, ~trains.columns.isin(drop_col)]\n",
    "test_y = trains[trains.time_id_ > 32767 * 0.8].target\n",
    "\n",
    "model = Lasso()\n",
    "model.fit(train_X, train_y)\n",
    "y_pred = model.predict(test_X)\n",
    "print(rmspe(test_y, y_pred))\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "random = RandomForestRegressor()\n",
    "random.fit(train_X, train_y)\n",
    "y_pred = random.predict(test_X)\n",
    "print(rmspe(test_y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -3.35252217e-11,\n",
       "        0.00000000e+00,  0.00000000e+00,  3.77211011e-11,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  2.16615558e-09,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains.to_csv('train.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44966cc58520d416351119416c640f1e9930367d91204e0da09aa706145b56ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
